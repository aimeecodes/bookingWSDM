{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### defining hyper-parameters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 92       # number of features in each time series step\n",
    "sequence_length = 2   # this will change with longer trips\n",
    "\n",
    "hidden_size = 128     # number of hidden nodes in each step\n",
    "num_layers = 2        # number of layers inside one 'pass'\n",
    "\n",
    "num_classes = 79      # number of countries seen in threestops\n",
    "num_epochs = 100      #\n",
    "batch_size = 16       #\n",
    "lr = 0.001            # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data loading###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration (not needed as my laptop contains no GPUs)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# set paths\n",
    "Xtrainpath = './threecountryXtrain.csv'\n",
    "Ytrainpath = './threecountryYtrain.csv'\n",
    "\n",
    "# load into pandas dataframe\n",
    "Xtraindf = pd.read_csv(Xtrainpath)\n",
    "Ytraindf = pd.read_csv(Ytrainpath)\n",
    "\n",
    "# drop the picked up index from importing\n",
    "Xtraindf.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "Ytraindf.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make tensors out of the training data\n",
    "Xtraintensor = torch.tensor(Xtraindf.values)\n",
    "Ytraintensor = torch.tensor(Ytraindf.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9627,     3,     0,  ...,     0,     0,     0],\n",
      "        [10332,     3,     1,  ...,     0,     0,     0],\n",
      "        [ 9337,     1,     0,  ...,     0,     0,     0],\n",
      "        ...,\n",
      "        [ 2436,     2,     1,  ...,     0,     0,     0],\n",
      "        [  359,     1,     0,  ...,     0,     0,     0],\n",
      "        [  359,     1,     0,  ...,     0,     0,     0]])\n",
      "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]])\n",
      "torch.Size([593])\n"
     ]
    }
   ],
   "source": [
    "print(Xtraintensor)\n",
    "print(Ytraintensor)\n",
    "\n",
    "# remove one-hot-encoding of Ytrain\n",
    "_, Ytrainvaluetensor = torch.max(Ytraintensor, dim=1)\n",
    "print(Ytrainvaluetensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save a copy of data which has been reshaped so all\n",
    "# sequence data is stored in one tensor (593 total examples)\n",
    "Xttgrouped = torch.reshape(Xtraintensor,(593, 92*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([593, 184])\n",
      "torch.Size([593, 79])\n"
     ]
    }
   ],
   "source": [
    "print(Xttgrouped.shape)\n",
    "print(Ytraintensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize empty array\n",
    "data_for_loader = []\n",
    "\n",
    "# fill it\n",
    "for i in range(0, len(Xttgrouped)):\n",
    "    data_for_loader.append((Xttgrouped[i], Ytrainvaluetensor[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 9627,     3,     0,     7,  5581,     1,     0,     0,     0,     0,\n",
       "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             1,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0, 10332,     3,     1,     7, 33177,     1,     0,     0,\n",
       "             0,     0,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     1,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0]),\n",
       " tensor(27))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_for_loader[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our data looks like it should, we can first split it into train and test sets, then pass these to the DataLoader class to batch over them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### splitting the data into train / test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify what proportion of the dataset should be included in training set\n",
    "train_proportion = 0.8\n",
    "\n",
    "train_size = int(train_proportion * len(data_for_loader))\n",
    "test_size = len(data_for_loader) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(data_for_loader, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset,\n",
    "                                          batch_size = batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: data_loader holds ALL of the data, now batched,\n",
    "# without a train / test split\n",
    "data_loader = torch.utils.data.DataLoader(data_for_loader,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2661,     1,     0,     6, 63454,     1,     0,     0,     0,     0,\n",
      "             0,     0,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0],\n",
      "        [  359,     2,     0,     6, 55763,     0,     1,     0,     0,     0,\n",
      "             0,     0,     1,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     1,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0]])\n",
      "tensor(12)\n"
     ]
    }
   ],
   "source": [
    "# check out the shape of the 'block' we're passing into our model\n",
    "for d in train_loader:\n",
    "    # d[0][0] should be our first length 2 sequence\n",
    "    print(d[0][0].reshape(sequence_length, input_size))\n",
    "    \n",
    "    # d[0][1] should be our first length 2 sequence's last country of the trip\n",
    "    print(d[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating the model object###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # X -> batch_size, sequence_length, input_size\n",
    "        # X -> -1, 2, 93\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # note: input x(0) is used because we specified batch_first=True\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size) # initial hidden state\n",
    "        \n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # out: batch_size, sequence_length, hidden_size\n",
    "        # out (-1, 2, 128)\n",
    "        out = out[:, -1, :]\n",
    "        # out (-1, 128)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train the model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()                  # applies the softmax for us\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                              lr = lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_steps = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 100, step 10/30, loss = 3.3913\n",
      "epoch 1 / 100, step 20/30, loss = 3.3444\n",
      "epoch 1 / 100, step 30/30, loss = 3.9164\n",
      "epoch 2 / 100, step 10/30, loss = 3.3081\n",
      "epoch 2 / 100, step 20/30, loss = 3.2332\n",
      "epoch 2 / 100, step 30/30, loss = 3.9551\n",
      "epoch 3 / 100, step 10/30, loss = 3.2423\n",
      "epoch 3 / 100, step 20/30, loss = 3.2010\n",
      "epoch 3 / 100, step 30/30, loss = 3.8223\n",
      "epoch 4 / 100, step 10/30, loss = 3.1705\n",
      "epoch 4 / 100, step 20/30, loss = 3.1940\n",
      "epoch 4 / 100, step 30/30, loss = 3.7260\n",
      "epoch 5 / 100, step 10/30, loss = 3.1427\n",
      "epoch 5 / 100, step 20/30, loss = 3.1881\n",
      "epoch 5 / 100, step 30/30, loss = 3.6487\n",
      "epoch 6 / 100, step 10/30, loss = 3.1080\n",
      "epoch 6 / 100, step 20/30, loss = 3.1857\n",
      "epoch 6 / 100, step 30/30, loss = 3.6032\n",
      "epoch 7 / 100, step 10/30, loss = 3.0706\n",
      "epoch 7 / 100, step 20/30, loss = 3.1799\n",
      "epoch 7 / 100, step 30/30, loss = 3.5331\n",
      "epoch 8 / 100, step 10/30, loss = 3.0528\n",
      "epoch 8 / 100, step 20/30, loss = 3.1639\n",
      "epoch 8 / 100, step 30/30, loss = 3.4923\n",
      "epoch 9 / 100, step 10/30, loss = 3.0153\n",
      "epoch 9 / 100, step 20/30, loss = 3.1552\n",
      "epoch 9 / 100, step 30/30, loss = 3.4745\n",
      "epoch 10 / 100, step 10/30, loss = 2.9615\n",
      "epoch 10 / 100, step 20/30, loss = 3.1728\n",
      "epoch 10 / 100, step 30/30, loss = 3.5121\n",
      "epoch 11 / 100, step 10/30, loss = 2.9013\n",
      "epoch 11 / 100, step 20/30, loss = 3.1323\n",
      "epoch 11 / 100, step 30/30, loss = 3.4223\n",
      "epoch 12 / 100, step 10/30, loss = 2.8523\n",
      "epoch 12 / 100, step 20/30, loss = 3.0934\n",
      "epoch 12 / 100, step 30/30, loss = 3.3764\n",
      "epoch 13 / 100, step 10/30, loss = 2.8465\n",
      "epoch 13 / 100, step 20/30, loss = 3.1026\n",
      "epoch 13 / 100, step 30/30, loss = 3.3236\n",
      "epoch 14 / 100, step 10/30, loss = 2.8355\n",
      "epoch 14 / 100, step 20/30, loss = 3.0600\n",
      "epoch 14 / 100, step 30/30, loss = 3.2680\n",
      "epoch 15 / 100, step 10/30, loss = 2.8276\n",
      "epoch 15 / 100, step 20/30, loss = 3.0455\n",
      "epoch 15 / 100, step 30/30, loss = 3.2778\n",
      "epoch 16 / 100, step 10/30, loss = 2.8500\n",
      "epoch 16 / 100, step 20/30, loss = 3.0479\n",
      "epoch 16 / 100, step 30/30, loss = 3.2642\n",
      "epoch 17 / 100, step 10/30, loss = 2.8541\n",
      "epoch 17 / 100, step 20/30, loss = 3.0068\n",
      "epoch 17 / 100, step 30/30, loss = 3.2605\n",
      "epoch 18 / 100, step 10/30, loss = 2.8336\n",
      "epoch 18 / 100, step 20/30, loss = 2.9761\n",
      "epoch 18 / 100, step 30/30, loss = 3.2764\n",
      "epoch 19 / 100, step 10/30, loss = 2.7870\n",
      "epoch 19 / 100, step 20/30, loss = 3.0165\n",
      "epoch 19 / 100, step 30/30, loss = 3.3378\n",
      "epoch 20 / 100, step 10/30, loss = 2.8298\n",
      "epoch 20 / 100, step 20/30, loss = 2.9543\n",
      "epoch 20 / 100, step 30/30, loss = 3.2153\n",
      "epoch 21 / 100, step 10/30, loss = 2.7830\n",
      "epoch 21 / 100, step 20/30, loss = 3.0324\n",
      "epoch 21 / 100, step 30/30, loss = 3.2035\n",
      "epoch 22 / 100, step 10/30, loss = 2.7140\n",
      "epoch 22 / 100, step 20/30, loss = 3.1235\n",
      "epoch 22 / 100, step 30/30, loss = 3.1773\n",
      "epoch 23 / 100, step 10/30, loss = 2.6680\n",
      "epoch 23 / 100, step 20/30, loss = 3.0188\n",
      "epoch 23 / 100, step 30/30, loss = 3.1497\n",
      "epoch 24 / 100, step 10/30, loss = 2.6923\n",
      "epoch 24 / 100, step 20/30, loss = 2.9908\n",
      "epoch 24 / 100, step 30/30, loss = 3.3215\n",
      "epoch 25 / 100, step 10/30, loss = 2.7341\n",
      "epoch 25 / 100, step 20/30, loss = 2.9536\n",
      "epoch 25 / 100, step 30/30, loss = 3.2752\n",
      "epoch 26 / 100, step 10/30, loss = 2.6717\n",
      "epoch 26 / 100, step 20/30, loss = 2.9488\n",
      "epoch 26 / 100, step 30/30, loss = 3.3522\n",
      "epoch 27 / 100, step 10/30, loss = 2.6832\n",
      "epoch 27 / 100, step 20/30, loss = 2.9854\n",
      "epoch 27 / 100, step 30/30, loss = 3.2002\n",
      "epoch 28 / 100, step 10/30, loss = 2.6795\n",
      "epoch 28 / 100, step 20/30, loss = 3.0505\n",
      "epoch 28 / 100, step 30/30, loss = 3.2147\n",
      "epoch 29 / 100, step 10/30, loss = 2.6249\n",
      "epoch 29 / 100, step 20/30, loss = 3.0816\n",
      "epoch 29 / 100, step 30/30, loss = 3.2039\n",
      "epoch 30 / 100, step 10/30, loss = 2.5873\n",
      "epoch 30 / 100, step 20/30, loss = 3.0401\n",
      "epoch 30 / 100, step 30/30, loss = 3.1723\n",
      "epoch 31 / 100, step 10/30, loss = 2.6046\n",
      "epoch 31 / 100, step 20/30, loss = 2.9972\n",
      "epoch 31 / 100, step 30/30, loss = 3.3790\n",
      "epoch 32 / 100, step 10/30, loss = 2.7097\n",
      "epoch 32 / 100, step 20/30, loss = 2.9933\n",
      "epoch 32 / 100, step 30/30, loss = 3.3346\n",
      "epoch 33 / 100, step 10/30, loss = 2.6296\n",
      "epoch 33 / 100, step 20/30, loss = 2.9698\n",
      "epoch 33 / 100, step 30/30, loss = 3.2227\n",
      "epoch 34 / 100, step 10/30, loss = 2.6396\n",
      "epoch 34 / 100, step 20/30, loss = 2.9492\n",
      "epoch 34 / 100, step 30/30, loss = 3.1409\n",
      "epoch 35 / 100, step 10/30, loss = 2.7309\n",
      "epoch 35 / 100, step 20/30, loss = 2.9204\n",
      "epoch 35 / 100, step 30/30, loss = 3.2602\n",
      "epoch 36 / 100, step 10/30, loss = 2.7104\n",
      "epoch 36 / 100, step 20/30, loss = 2.9281\n",
      "epoch 36 / 100, step 30/30, loss = 3.0946\n",
      "epoch 37 / 100, step 10/30, loss = 2.6832\n",
      "epoch 37 / 100, step 20/30, loss = 2.9164\n",
      "epoch 37 / 100, step 30/30, loss = 3.0207\n",
      "epoch 38 / 100, step 10/30, loss = 2.6215\n",
      "epoch 38 / 100, step 20/30, loss = 2.9195\n",
      "epoch 38 / 100, step 30/30, loss = 3.0761\n",
      "epoch 39 / 100, step 10/30, loss = 2.7606\n",
      "epoch 39 / 100, step 20/30, loss = 2.9052\n",
      "epoch 39 / 100, step 30/30, loss = 3.0983\n",
      "epoch 40 / 100, step 10/30, loss = 2.6870\n",
      "epoch 40 / 100, step 20/30, loss = 2.8982\n",
      "epoch 40 / 100, step 30/30, loss = 3.1335\n",
      "epoch 41 / 100, step 10/30, loss = 2.7153\n",
      "epoch 41 / 100, step 20/30, loss = 2.9597\n",
      "epoch 41 / 100, step 30/30, loss = 3.1453\n",
      "epoch 42 / 100, step 10/30, loss = 2.6763\n",
      "epoch 42 / 100, step 20/30, loss = 3.0039\n",
      "epoch 42 / 100, step 30/30, loss = 3.1166\n",
      "epoch 43 / 100, step 10/30, loss = 2.6637\n",
      "epoch 43 / 100, step 20/30, loss = 2.9613\n",
      "epoch 43 / 100, step 30/30, loss = 3.0565\n",
      "epoch 44 / 100, step 10/30, loss = 2.6252\n",
      "epoch 44 / 100, step 20/30, loss = 2.9191\n",
      "epoch 44 / 100, step 30/30, loss = 3.0644\n",
      "epoch 45 / 100, step 10/30, loss = 2.6334\n",
      "epoch 45 / 100, step 20/30, loss = 2.9123\n",
      "epoch 45 / 100, step 30/30, loss = 3.0302\n",
      "epoch 46 / 100, step 10/30, loss = 2.5376\n",
      "epoch 46 / 100, step 20/30, loss = 2.9160\n",
      "epoch 46 / 100, step 30/30, loss = 3.1704\n",
      "epoch 47 / 100, step 10/30, loss = 2.6025\n",
      "epoch 47 / 100, step 20/30, loss = 2.9214\n",
      "epoch 47 / 100, step 30/30, loss = 3.3590\n",
      "epoch 48 / 100, step 10/30, loss = 2.7289\n",
      "epoch 48 / 100, step 20/30, loss = 2.8795\n",
      "epoch 48 / 100, step 30/30, loss = 3.5098\n",
      "epoch 49 / 100, step 10/30, loss = 2.7567\n",
      "epoch 49 / 100, step 20/30, loss = 2.8198\n",
      "epoch 49 / 100, step 30/30, loss = 3.3824\n",
      "epoch 50 / 100, step 10/30, loss = 2.6851\n",
      "epoch 50 / 100, step 20/30, loss = 2.8288\n",
      "epoch 50 / 100, step 30/30, loss = 3.2071\n",
      "epoch 51 / 100, step 10/30, loss = 2.7151\n",
      "epoch 51 / 100, step 20/30, loss = 2.8259\n",
      "epoch 51 / 100, step 30/30, loss = 3.1869\n",
      "epoch 52 / 100, step 10/30, loss = 2.6217\n",
      "epoch 52 / 100, step 20/30, loss = 2.8482\n",
      "epoch 52 / 100, step 30/30, loss = 3.1667\n",
      "epoch 53 / 100, step 10/30, loss = 2.5714\n",
      "epoch 53 / 100, step 20/30, loss = 2.8580\n",
      "epoch 53 / 100, step 30/30, loss = 3.1430\n",
      "epoch 54 / 100, step 10/30, loss = 2.5432\n",
      "epoch 54 / 100, step 20/30, loss = 2.8760\n",
      "epoch 54 / 100, step 30/30, loss = 3.1188\n",
      "epoch 55 / 100, step 10/30, loss = 2.6413\n",
      "epoch 55 / 100, step 20/30, loss = 2.8703\n",
      "epoch 55 / 100, step 30/30, loss = 3.1128\n",
      "epoch 56 / 100, step 10/30, loss = 2.6418\n",
      "epoch 56 / 100, step 20/30, loss = 2.8482\n",
      "epoch 56 / 100, step 30/30, loss = 3.0992\n",
      "epoch 57 / 100, step 10/30, loss = 2.6607\n",
      "epoch 57 / 100, step 20/30, loss = 2.8252\n",
      "epoch 57 / 100, step 30/30, loss = 3.0970\n",
      "epoch 58 / 100, step 10/30, loss = 2.6667\n",
      "epoch 58 / 100, step 20/30, loss = 2.7881\n",
      "epoch 58 / 100, step 30/30, loss = 3.0985\n",
      "epoch 59 / 100, step 10/30, loss = 2.6878\n",
      "epoch 59 / 100, step 20/30, loss = 2.8064\n",
      "epoch 59 / 100, step 30/30, loss = 3.0713\n",
      "epoch 60 / 100, step 10/30, loss = 2.6182\n",
      "epoch 60 / 100, step 20/30, loss = 2.8593\n",
      "epoch 60 / 100, step 30/30, loss = 3.2735\n",
      "epoch 61 / 100, step 10/30, loss = 2.7009\n",
      "epoch 61 / 100, step 20/30, loss = 2.8007\n",
      "epoch 61 / 100, step 30/30, loss = 3.3303\n",
      "epoch 62 / 100, step 10/30, loss = 2.6702\n",
      "epoch 62 / 100, step 20/30, loss = 2.7974\n",
      "epoch 62 / 100, step 30/30, loss = 3.1046\n",
      "epoch 63 / 100, step 10/30, loss = 2.5814\n",
      "epoch 63 / 100, step 20/30, loss = 2.7729\n",
      "epoch 63 / 100, step 30/30, loss = 3.0245\n",
      "epoch 64 / 100, step 10/30, loss = 2.5234\n",
      "epoch 64 / 100, step 20/30, loss = 2.7628\n",
      "epoch 64 / 100, step 30/30, loss = 3.0375\n",
      "epoch 65 / 100, step 10/30, loss = 2.5114\n",
      "epoch 65 / 100, step 20/30, loss = 2.8373\n",
      "epoch 65 / 100, step 30/30, loss = 3.1264\n",
      "epoch 66 / 100, step 10/30, loss = 2.5303\n",
      "epoch 66 / 100, step 20/30, loss = 2.8891\n",
      "epoch 66 / 100, step 30/30, loss = 3.0582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 67 / 100, step 10/30, loss = 2.4670\n",
      "epoch 67 / 100, step 20/30, loss = 2.9204\n",
      "epoch 67 / 100, step 30/30, loss = 3.0643\n",
      "epoch 68 / 100, step 10/30, loss = 2.4330\n",
      "epoch 68 / 100, step 20/30, loss = 2.8966\n",
      "epoch 68 / 100, step 30/30, loss = 3.0441\n",
      "epoch 69 / 100, step 10/30, loss = 2.4323\n",
      "epoch 69 / 100, step 20/30, loss = 2.8678\n",
      "epoch 69 / 100, step 30/30, loss = 3.0053\n",
      "epoch 70 / 100, step 10/30, loss = 2.4068\n",
      "epoch 70 / 100, step 20/30, loss = 2.8273\n",
      "epoch 70 / 100, step 30/30, loss = 3.0213\n",
      "epoch 71 / 100, step 10/30, loss = 2.5584\n",
      "epoch 71 / 100, step 20/30, loss = 2.8385\n",
      "epoch 71 / 100, step 30/30, loss = 3.3524\n",
      "epoch 72 / 100, step 10/30, loss = 2.5266\n",
      "epoch 72 / 100, step 20/30, loss = 2.8241\n",
      "epoch 72 / 100, step 30/30, loss = 3.2772\n",
      "epoch 73 / 100, step 10/30, loss = 2.5302\n",
      "epoch 73 / 100, step 20/30, loss = 2.8496\n",
      "epoch 73 / 100, step 30/30, loss = 3.2847\n",
      "epoch 74 / 100, step 10/30, loss = 2.5935\n",
      "epoch 74 / 100, step 20/30, loss = 2.8402\n",
      "epoch 74 / 100, step 30/30, loss = 3.1810\n",
      "epoch 75 / 100, step 10/30, loss = 2.5916\n",
      "epoch 75 / 100, step 20/30, loss = 2.8009\n",
      "epoch 75 / 100, step 30/30, loss = 3.1603\n",
      "epoch 76 / 100, step 10/30, loss = 2.6258\n",
      "epoch 76 / 100, step 20/30, loss = 2.8196\n",
      "epoch 76 / 100, step 30/30, loss = 3.1054\n",
      "epoch 77 / 100, step 10/30, loss = 2.6526\n",
      "epoch 77 / 100, step 20/30, loss = 2.7812\n",
      "epoch 77 / 100, step 30/30, loss = 3.0839\n",
      "epoch 78 / 100, step 10/30, loss = 2.6838\n",
      "epoch 78 / 100, step 20/30, loss = 2.8760\n",
      "epoch 78 / 100, step 30/30, loss = 2.8971\n",
      "epoch 79 / 100, step 10/30, loss = 2.5848\n",
      "epoch 79 / 100, step 20/30, loss = 2.8261\n",
      "epoch 79 / 100, step 30/30, loss = 2.9007\n",
      "epoch 80 / 100, step 10/30, loss = 2.5695\n",
      "epoch 80 / 100, step 20/30, loss = 2.9013\n",
      "epoch 80 / 100, step 30/30, loss = 2.9130\n",
      "epoch 81 / 100, step 10/30, loss = 2.5494\n",
      "epoch 81 / 100, step 20/30, loss = 2.9032\n",
      "epoch 81 / 100, step 30/30, loss = 2.8549\n",
      "epoch 82 / 100, step 10/30, loss = 2.5336\n",
      "epoch 82 / 100, step 20/30, loss = 2.8237\n",
      "epoch 82 / 100, step 30/30, loss = 2.8376\n",
      "epoch 83 / 100, step 10/30, loss = 2.4808\n",
      "epoch 83 / 100, step 20/30, loss = 2.7876\n",
      "epoch 83 / 100, step 30/30, loss = 2.8199\n",
      "epoch 84 / 100, step 10/30, loss = 2.4592\n",
      "epoch 84 / 100, step 20/30, loss = 2.7289\n",
      "epoch 84 / 100, step 30/30, loss = 2.8641\n",
      "epoch 85 / 100, step 10/30, loss = 2.4879\n",
      "epoch 85 / 100, step 20/30, loss = 2.8046\n",
      "epoch 85 / 100, step 30/30, loss = 2.7732\n",
      "epoch 86 / 100, step 10/30, loss = 2.5252\n",
      "epoch 86 / 100, step 20/30, loss = 2.7640\n",
      "epoch 86 / 100, step 30/30, loss = 2.7695\n",
      "epoch 87 / 100, step 10/30, loss = 2.4418\n",
      "epoch 87 / 100, step 20/30, loss = 2.7049\n",
      "epoch 87 / 100, step 30/30, loss = 2.8374\n",
      "epoch 88 / 100, step 10/30, loss = 2.4837\n",
      "epoch 88 / 100, step 20/30, loss = 2.7017\n",
      "epoch 88 / 100, step 30/30, loss = 2.8495\n",
      "epoch 89 / 100, step 10/30, loss = 2.4783\n",
      "epoch 89 / 100, step 20/30, loss = 2.7433\n",
      "epoch 89 / 100, step 30/30, loss = 2.8146\n",
      "epoch 90 / 100, step 10/30, loss = 2.4566\n",
      "epoch 90 / 100, step 20/30, loss = 2.7423\n",
      "epoch 90 / 100, step 30/30, loss = 2.7632\n",
      "epoch 91 / 100, step 10/30, loss = 2.4404\n",
      "epoch 91 / 100, step 20/30, loss = 2.7301\n",
      "epoch 91 / 100, step 30/30, loss = 2.7522\n",
      "epoch 92 / 100, step 10/30, loss = 2.4676\n",
      "epoch 92 / 100, step 20/30, loss = 2.7083\n",
      "epoch 92 / 100, step 30/30, loss = 2.8354\n",
      "epoch 93 / 100, step 10/30, loss = 2.4437\n",
      "epoch 93 / 100, step 20/30, loss = 2.6966\n",
      "epoch 93 / 100, step 30/30, loss = 2.8202\n",
      "epoch 94 / 100, step 10/30, loss = 2.4379\n",
      "epoch 94 / 100, step 20/30, loss = 2.6663\n",
      "epoch 94 / 100, step 30/30, loss = 2.8069\n",
      "epoch 95 / 100, step 10/30, loss = 2.4308\n",
      "epoch 95 / 100, step 20/30, loss = 2.6753\n",
      "epoch 95 / 100, step 30/30, loss = 2.7915\n",
      "epoch 96 / 100, step 10/30, loss = 2.4317\n",
      "epoch 96 / 100, step 20/30, loss = 2.6694\n",
      "epoch 96 / 100, step 30/30, loss = 2.7932\n",
      "epoch 97 / 100, step 10/30, loss = 2.4185\n",
      "epoch 97 / 100, step 20/30, loss = 2.6410\n",
      "epoch 97 / 100, step 30/30, loss = 2.7803\n",
      "epoch 98 / 100, step 10/30, loss = 2.4113\n",
      "epoch 98 / 100, step 20/30, loss = 2.6321\n",
      "epoch 98 / 100, step 30/30, loss = 2.6741\n",
      "epoch 99 / 100, step 10/30, loss = 2.4346\n",
      "epoch 99 / 100, step 20/30, loss = 2.6260\n",
      "epoch 99 / 100, step 30/30, loss = 2.6792\n",
      "epoch 100 / 100, step 10/30, loss = 2.4166\n",
      "epoch 100 / 100, step 20/30, loss = 2.6125\n",
      "epoch 100 / 100, step 30/30, loss = 2.7423\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (Xtrain, Ytrain) in enumerate(train_loader):\n",
    "        \n",
    "        # reshape the data as needed\n",
    "        Xtrain = Xtrain.reshape(-1, sequence_length, input_size).float()\n",
    "        \n",
    "        # forward\n",
    "        outputs = model(Xtrain)\n",
    "        loss = criterion(outputs, Ytrain)\n",
    "        \n",
    "        # backwards\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 10 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epochs}, step {i+1}/{n_total_steps}, loss = {loss.item():.4f}')       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 15.126050420168067\n"
     ]
    }
   ],
   "source": [
    "# stop computing gradients for memory efficiency\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for i, (Xtest, Ytest) in enumerate(test_loader):\n",
    "        \n",
    "        # reshape the data as needed\n",
    "        Xtest = Xtest.reshape(-1, sequence_length, input_size).float()\n",
    "        \n",
    "        outputs = model(Xtest)\n",
    "        \n",
    "        # returns value, index\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        \n",
    "        n_samples += Xtest.shape[0]\n",
    "        \n",
    "        n_correct += (predictions == Ytest).sum().item()\n",
    "        \n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'accuracy = {acc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
